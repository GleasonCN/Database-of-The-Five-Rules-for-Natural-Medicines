import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, recall_score, precision_score, matthews_corrcoef, confusion_matrix
import seaborn as sns
import shap

# Reading data from Excel files
data = pd.read_excel('#Import your file#.xlsx')

# Assume that the second to the second to last columns are features and the last column is the label
features = data.iloc[:, 1:-1]
labels = data.iloc[:, -1]

# Scaling Data
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Divide the dataset
X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2, random_state=42)

# Initialize the model and hyperparameters
model_params = {
    'Random Forest': {
        'model': RandomForestClassifier(random_state=42),
        'params': {'n_estimators': [50, 100, 200]}
    },
    'Gradient Boosting': {
        'model': GradientBoostingClassifier(random_state=42),
        'params': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.5]}
    },
    'Logistic Regression': {
        'model': LogisticRegression(max_iter=1000, random_state=42),
        'params': {'C': [0.1, 1, 10]}
    },
    'Support Vector Machine': {
        'model': SVC(probability=True, random_state=42),
        'params': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
    }
}

# Training and evaluating the model
results = {}
best_accuracy = 0
best_model_name = None
best_model = None

for model_name, mp in model_params.items():
    grid_search = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, scoring='accuracy')
    grid_search.fit(X_train, y_train)

    best_clf = grid_search.best_estimator_
    y_pred = best_clf.predict(X_test)
    y_pred_proba = best_clf.predict_proba(X_test)[:, 1] if hasattr(best_clf, "predict_proba") else best_clf.decision_function(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    auc = roc_auc_score(y_test, y_pred_proba)
    sensitivity = recall_score(y_test, y_pred)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    specificity = tn / (tn + fp)
    precision = precision_score(y_test, y_pred)
    mcc = matthews_corrcoef(y_test, y_pred)

    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)

    results[model_name] = {
        "best_params": grid_search.best_params_,
        "model": best_clf,
        "accuracy": accuracy,
        "f1_score": f1,
        "auc": auc,
        "sensitivity": sensitivity,
        "specificity": specificity,
        "precision": precision,
        "mcc": mcc,
        "fpr": fpr,
        "tpr": tpr,
        "confusion_matrix": confusion_matrix(y_test, y_pred)
    }

    if model_name == 'Random Forest':
        best_accuracy = accuracy
        best_model_name = model_name
        best_model = best_clf

# Printing Results
for model_name, metrics in results.items():
    print(f"Model: {model_name}")
    print(f"Best Params: {metrics['best_params']}")
    print(f"Accuracy: {metrics['accuracy']}")
    print(f"F1 Score: {metrics['f1_score']}")
    print(f"AUC: {metrics['auc']}")
    print(f"Sensitivity: {metrics['sensitivity']}")
    print(f"Specificity: {metrics['specificity']}")
    print(f"Precision: {metrics['precision']}")
    print(f"MCC: {metrics['mcc']}")
    print("=" * 30)

# Draw an improved ROC curve
plt.figure(figsize=(10, 8))
for model_name in results.keys():
    plt.plot(results[model_name]['fpr'], results[model_name]['tpr'],
             label=f"{model_name} (AUC = {results[model_name]['auc']:.2f})", lw=2)
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlabel('False Positive Rate', fontsize=14)
plt.ylabel('True Positive Rate', fontsize=14)
plt.title('ROC Curve', fontsize=16)
plt.legend(loc='best', fontsize=12)
plt.grid(True, linestyle='--', linewidth=0.5)
plt.show()

# Improved confusion matrix visualization
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
axes = axes.flatten()
for i, (model_name, metrics) in enumerate(results.items()):
    cm = metrics['confusion_matrix']
    sns.heatmap(cm, annot=True, fmt='d', ax=axes[i], cmap='Blues', cbar=False, annot_kws={"size": 14}, linewidths=0.5, linecolor='black')
    axes[i].set_title(f"Confusion Matrix: {model_name}", fontsize=16)
    axes[i].set_xlabel('Predicted', fontsize=14)
    axes[i].set_ylabel('Actual', fontsize=14)
    axes[i].set_xticklabels(axes[i].get_xticklabels(), fontsize=12)
    axes[i].set_yticklabels(axes[i].get_yticklabels(), fontsize=12)
plt.tight_layout()
plt.show()

# Interpreting SHAP feature importance for random forest
explainer = shap.TreeExplainer(results['Random Forest']['model'])
shap_values = explainer.shap_values(X_train)

# If shap_values ​​is a list, a non-baseline index is selected.
if isinstance(shap_values, list) and len(shap_values) > 1:
    shap_values_for_plot = shap_values[1]
else:
    shap_values_for_plot = shap_values

# Plotting SHAP feature importance of the top 10 features
shap.summary_plot(shap_values_for_plot, X_train, feature_names=features.columns, max_display=10)

# Calculate the average importance of all features
shap_mean_abs = np.mean(np.abs(shap_values_for_plot), axis=0)
shap_importance_df = pd.DataFrame({
    'Feature': features.columns,
    'Importance': shap_mean_abs
})

# Sort by importance and export to Excel file
shap_importance_df = shap_importance_df.sort_values(by='Importance', ascending=False)
shap_importance_df.to_excel('shap_feature_importance.xlsx', index=False)

print("The SHAP feature importance has been saved into the 'shap_feature_importance.xlsx' file.")

# Save the evaluation indicators of all models to Excel files
metrics_df = pd.DataFrame.from_dict(results, orient='index')
metrics_df = metrics_df.reset_index().rename(columns={'index': 'Model'})
metrics_df.to_excel('model_evaluation_metrics.xlsx', index=False)

print("The evaluation metrics for all models have been saved into the 'model_evaluation_metrics.xlsx' file.")
